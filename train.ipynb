{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型訓練階段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------引入基本套件-----------#\n",
    "import time\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import cv2\n",
    "from keras import backend as K\n",
    "from keras.callbacks import (EarlyStopping, ModelCheckpoint, ReduceLROnPlateau,\n",
    "                             TensorBoard)\n",
    "from keras.metrics import categorical_accuracy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.utils.data_utils import get_file\n",
    "from PIL import Image   \n",
    "#-----------引入模型架構-----------#\n",
    "from nets.generator_training import *  #引入.fir_generator所需要使用的生成器\n",
    "from nets.unet import *                         #從net/unet.py 中引入Unet模型，內有Unet3、4、5、6\n",
    "from nets.FCN8 import *                       #FCN8\n",
    "from nets.pspnet import pspnet           #PSPNET\n",
    "#-----引入損失函數&評價函數-----#\n",
    "from utils.loss import *       #調用自訂義的損失函數\n",
    "from utils.metrics import *  #調用自訂義的評價函數\n",
    "#-----限制顯示卡記憶體用量-----#\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "if len(gpus) != 0:  # 若有檢測到GPU，則設定使用第一張GPU，並設定記憶體上限7GB\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=7168)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全域變數\n",
    "使用這裡的變數來調整參數，以下做小規模測試方便理解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------控制輸入圖片---------#\n",
    "RA = False  #True則代表輸入該圖片\n",
    "RD = True\n",
    "IR = True\n",
    "GI = False\n",
    "#---------控制雨量區間---------#\n",
    "interval = 'EWB01' #EWB01共100個標籤\n",
    "num_classes = 100\n",
    "#---------控制移動時間--------#\n",
    "shift = 1                        #預測 t+shift 時刻\n",
    "t2t = 't2t+'+str(shift)\n",
    "#--------控制訓練的參數------#\n",
    "Freeze_Epoch = 100            #訓練輪次\n",
    "Batch_size = 8                   #批次大小\n",
    "focal_loss = False              #控制是否使用focal loss，False則 = 'categorical_crossentropy'\n",
    "choose_model = 'Unet5'   #模型名稱"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練模型\n",
    "計算完的權重會儲存至checkpoint資料夾中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_start = time.time()  #計時器\n",
    "if __name__ == \"__main__\":   \n",
    "    K.clear_session()  #清空權重\n",
    "    log_dir = \"logs/\"\n",
    "    #   輸入圖片的通道數，根據不同參數有不同通道數or圖片大小\n",
    "    if RD==True and IR==False and RA==False:   #RD\n",
    "        inputs_size = [128,128,3]\n",
    "    elif RD==False and IR==False and RA==True:   #RA\n",
    "        inputs_size = [128,128,3]\n",
    "    elif RD==False and IR==True and RA==False:  #IR\n",
    "        inputs_size = [128,128,3]\n",
    "    elif RD==True and IR==True and RA==False:  #RD+IR\n",
    "        inputs_size = [128,128,4]\n",
    "    elif RD==True and IR==False and RA==True:  #RD+RA\n",
    "        inputs_size = [128,128,6]\n",
    "    elif RD==False and IR==True and RA==True:  #IR+RA\n",
    "        inputs_size = [128,128,4]\n",
    "    elif RD==True and IR==True and RA==True:  #RD+IR+RA\n",
    "        inputs_size = [128,128,7]\n",
    "        \n",
    "    if GI == True:\n",
    "        inputs_size[2] = inputs_size[2] +2  #channel+2，GI加在最後一層\n",
    "\n",
    "    # 選擇要用的model\n",
    "    if choose_model == 'Unet6':\n",
    "        model = Unet6(inputs_size,num_classes)\n",
    "    elif choose_model == 'Unet5':\n",
    "        model = Unet5(inputs_size,num_classes)\n",
    "    elif choose_model == 'Unet4':\n",
    "        model = Unet4(inputs_size,num_classes)\n",
    "    elif choose_model == 'Unet3':\n",
    "        model = Unet3(inputs_size,num_classes)\n",
    "    elif choose_model == 'FCN8':\n",
    "        model = FCN8(inputs_size,num_classes)\n",
    "    elif choose_model == 'pspnet':\n",
    "        model = pspnet(inputs_size,num_classes)\n",
    "    #-------------------------------------------#\n",
    "    # 打開數據集的訓練.txt  \n",
    "    with open(r\"database/train_list/\"+t2t+\"/train.txt\",\"r\") as f:\n",
    "        train_lines = f.readlines()\n",
    "    # 打開數據集的驗證.txt \n",
    "    with open(r\"database/train_list/\"+t2t+\"/val.txt\",\"r\") as f:\n",
    "        val_lines = f.readlines()\n",
    "    #-------------------------------------------------------------------------------#\n",
    "    #   訓練參數的設置\n",
    "    #   logging表示tensorboard的保存位址\n",
    "    #   checkpoint用於設置權值保存的細節，period用於修改多少epoch保存一次\n",
    "    #   reduce_lr用於設置學習率下降的方式\n",
    "    #   early_stopping用於設置早停，val_loss多次不下降自動結束訓練，表示模型基本收斂\n",
    "    #-------------------------------------------------------------------------------#\n",
    "    checkpoint_period = ModelCheckpoint('checkpoint/'+choose_model+'_val_min_RMSE.h5',\n",
    "                                    monitor='val__RMSE', save_weights_only=True, save_best_only=True, verbose=1, period=1,\n",
    "                                       mode='min') #若更換評價指標須注意模式的問題\n",
    "    #reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1,mode='min')\n",
    "    #factor 降低學習率的因數 lr = lr * factor \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=15, verbose=1,mode='min',restore_best_weights=True)\n",
    "    #monitor 監測的數據對像；patience 10輪沒有進步訓練便停止；restore_best_weight=True讓最後輸出的模型是回復到前10輪最佳的那個模型\n",
    "    tensorboard = TensorBoard(log_dir=log_dir)\n",
    "    #------------------------------------------------------#\n",
    "    #   提示OOM或者顯存不足請調小Batch_size\n",
    "    #------------------------------------------------------#\n",
    "    alpha = []\n",
    "    for i in range(num_classes-len(alpha)):  #循環加入標籤權重，從1~num_class\n",
    "        alpha.append([i+1])\n",
    "    #alpha為focal loss 1所使用之函數，各類別的加權\n",
    "    if True:\n",
    "        lr = 1e-4\n",
    "        Init_Epoch = 0\n",
    "        #Freeze_Epoch = 100\n",
    "        #Batch_size = 16\n",
    "        #判斷是否要使用focal loss，若不用則使用交叉熵'categorical_crossentropy'\n",
    "        model.compile(loss =multi_category_focal_loss1(alpha=alpha,gamma=2.0) if focal_loss else 'categorical_crossentropy',\n",
    "                optimizer = Adam(lr=lr),\n",
    "                metrics = [Iou_score(),f_score(), precision(),recall(),'acc',mean_RMSE(),blank_RMSE()])\n",
    "        print('Train on {} samples, val on {} samples, with batch size {}.'.format(len(train_lines), len(val_lines), Batch_size))\n",
    "        \n",
    "        gen = Generator_merge(Batch_size, train_lines, inputs_size, num_classes,RD=RD,IR=IR,GI=GI,RA=RA,interval=interval).generate()  #訓練集\n",
    "        gen_val = Generator_merge_val(1, val_lines, inputs_size, num_classes,RD=RD,IR=IR,GI=GI,RA=RA,interval=interval).generate() #驗證集 \n",
    "        #gen_val 批次=1是為了計算驗證時能計算到所有的資料(使用batch size=1遍歷整個驗證集)\n",
    "        history = model.fit_generator(gen,\n",
    "                steps_per_epoch=max(1, len(train_lines)//Batch_size),\n",
    "                validation_data=gen_val,\n",
    "                validation_steps= len(val_lines),  #驗證步伐=1所以驗證步數要=len(val) 資料長度\n",
    "                epochs=Freeze_Epoch,\n",
    "                initial_epoch=Init_Epoch,\n",
    "                callbacks=[checkpoint_period,tensorboard,early_stopping])\n",
    "        #model.save_weights('model_data/unet_weights_rain.h5')  #保存最後一個模型權重\n",
    "        #model.save('model_data/unet_full_rain.h5')  #保存最後一個模型\n",
    "time_end = time.time()\n",
    "time_c= time_end - time_start   #執行所花時間\n",
    "print('time cost', time_c, 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "調出history中的資料，並儲存成csv檔案保存\n",
    "最終會將model_history輸出csv和折線圖至output資料夾中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "IOU = history.history[\"_Iou_score\"]\n",
    "val_IOU = history.history[\"val__Iou_score\"]\n",
    "f_score = history.history[\"_f_score\"]\n",
    "val_f_score = history.history[\"val__f_score\"]\n",
    "precision = history.history[\"metric_precision\"]\n",
    "val_precision = history.history[\"val_metric_precision\"]\n",
    "recall = history.history[\"metric_recall\"]\n",
    "val_recall = history.history[\"val_metric_recall\"]\n",
    "RMSE = history.history[\"_RMSE\"]\n",
    "val_RMSE = history.history[\"val__RMSE\"]\n",
    "acc = history.history[\"acc\"]\n",
    "val_acc = history.history[\"val_acc\"]\n",
    "\n",
    "epoch = history.epoch #輪次\n",
    "merge=np.vstack((epoch,loss,val_loss,IOU,val_IOU,f_score,val_f_score,precision,val_precision,\n",
    "                 recall,val_recall,RMSE,val_RMSE,acc,val_acc))   #沿豎直方向將矩陣堆疊起來\n",
    "merge=pd.DataFrame(merge)  #轉成df\n",
    "merge = merge.T  #轉置\n",
    "merge.columns=['epoch','train_loss','val_loss','train_IOU','val_IOU','train__f_score','val__f_score',\n",
    "               'train_precision','val_precision','train_recall','val_recall','train_RMSE','val_RMSE',\n",
    "              'train_acc','val_acc']  #欄位名稱命名\n",
    "merge.to_csv(\"./output/model_history_\"+t2t+\"_\"+interval+\"_\"+choose_model+\".csv\",index=0) #儲存成csv檔案\n",
    "\"\"\"繪製val&loss曲線\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(12, figsize=(7, 24), dpi=60)\n",
    "plt.subplot(711)\n",
    "plt.plot(loss, label='train')\n",
    "plt.plot(val_loss, label='val')\n",
    "plt.title('loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(712)\n",
    "plt.plot(IOU, label='train')\n",
    "plt.plot(val_IOU, label='val')\n",
    "plt.title('Iou_score')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(713)\n",
    "plt.plot(f_score, label='train')\n",
    "plt.plot(val_f_score, label='val')\n",
    "plt.title('f-score')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(714)\n",
    "plt.plot(precision, label='train')\n",
    "plt.plot(val_precision, label='val')\n",
    "plt.title('precision')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(715)\n",
    "plt.plot(recall, label='train')\n",
    "plt.plot(val_recall, label='val')\n",
    "plt.title('recall')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(716)\n",
    "plt.plot(RMSE, label='train')\n",
    "plt.plot(val_RMSE, label='val')\n",
    "plt.title('RMSE')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(717)\n",
    "plt.plot(acc, label='train')\n",
    "plt.plot(val_acc, label='val')\n",
    "plt.title('acc')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(\"./output/model_history_\"+t2t+\"_\"+interval+\"_\"+choose_model+\".png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 輸出預測的圖像與二維陣列.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"#predict 【針對單一輪次輸出預測(圖像、陣列)】\n",
    "#EWB01、EWB10要使用unet.detect_image_merge_label輸出二維陣列到output/predict_array資料夾中\n",
    "並在計算完輸出CWB18的雨量分布圖做可視化的參考\n",
    "\"\"\"\n",
    "from utils.model_output import *\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "if not os.path.isdir('./output'):  #若該資料夾不存在則生成資料夾\n",
    "    os.mkdir('./output')\n",
    "if not os.path.isdir('./output/predict'):\n",
    "    os.mkdir('./output/predict')\n",
    "if not os.path.isdir('./output/predict_array'):\n",
    "    os.mkdir('./output/predict_array')  \n",
    "checkpoint = glob.glob('./checkpoint/*.h5')  #glob取出checkpoint所有檔名\n",
    "for last_checkpoint in checkpoint:  #走訪checkpoint\n",
    "    None #走完便會將最後一個checkpoint檔名存入llast_checkpoint中\n",
    "print(last_checkpoint)\n",
    "#last_checkpoint = './checkpoint/ep075-loss0.624-val_loss1.536.h5' #手動調用想要觀察的模型\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    K.clear_session()  #清空權重非常重要\n",
    "    last_model_weights = False   #是否要調用最後一個模型，True = last；False=best\n",
    "    blend = False# blend參數用於控制是否讓識別結果和原圖混合  False不混合(輸出純白背景)\n",
    "\n",
    "    if RD==True and IR==False and RA==False:   #RD\n",
    "        model_image_size = [128,128,3]\n",
    "    elif RD==False and IR==False and RA==True:   #RA\n",
    "        model_image_size = [128,128,3]\n",
    "    elif RD==False and IR==True and RA==False:  #IR\n",
    "        model_image_size = [128,128,3]\n",
    "    elif RD==True and IR==True and RA==False:  #RD+IR\n",
    "        model_image_size = [128,128,4]\n",
    "    elif RD==True and IR==False and RA==True:  #RD+RA\n",
    "        model_image_size = [128,128,6]\n",
    "    elif RD==False and IR==True and RA==True:  #IR+RA\n",
    "        model_image_size = [128,128,4]\n",
    "    elif RD==True and IR==True and RA==True:  #RD+IR+RA\n",
    "        model_image_size = [128,128,7]\n",
    "    if GI == True:\n",
    "        model_image_size[2] = model_image_size[2] +2  #channel+2，GI加在最後一層\n",
    "    \n",
    "    model_path = last_checkpoint  #最佳權重\n",
    "    \n",
    "    #變數unet 和 函數Unet()僅為代稱，詳情參見model_output.py，內部整合了所有模型的輸出方式\n",
    "    #將根據choose_model用於選取對應的模型參數\n",
    "    unet = Unet(model_path=model_path,model_image_size=model_image_size,num_classes=num_classes,\n",
    "                blend=blend,RD=RD,IR=IR,GI=GI,RA=RA,choose_model=choose_model)\n",
    "    \n",
    "    val = pd.read_csv('./database/train_list/'+t2t+'/val.txt',header=None,delimiter=';') #讀取val資料夾用於對應計算\n",
    "    pre=val[:][0]  #要預測的檔名於第0行中\n",
    "    for jpg in range(len(pre)):\n",
    "        RD_image = Image.open(\"./database/train_data_RD/val/\"+pre[jpg])#匯入雷達圖像\n",
    "        IR_image = Image.open(\"./database/train_data_IR/val/\"+pre[jpg])#匯入IR圖像\n",
    "        RA_image = Image.open(\"./database/train_data_RA/\"+interval+\"/val/\"+ pre[jpg]) #匯入雨量分布圖(標籤)\n",
    "        \n",
    "        #反正都丟進去，內部在根據RA+RD+IR+GI的組合去判斷要使用哪些資料\n",
    "        pre_tmp = pre[jpg] #取出第k個標籤，加上t+shift\n",
    "        pre_tmp = pre_tmp[:-4]+'_t+'+str(shift)  #將標籤的檔名後四位刪除 (刪除.png) 因為標籤為csv形式\n",
    "        \n",
    "        #detect_image_merge_label用於輸出預測出來的陣列\n",
    "        r_label = unet.detect_image_merge_label(RD_image,IR_image,RA_image)\n",
    "        np.savetxt(\"./output/predict_array/\"+pre_tmp+'.csv', r_label, delimiter=\",\",fmt='%d',) #預測的值儲存成csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "計算【單一輪次的誤差】(標籤轉相對真值與真值相減)，讀取的是predict_array內的資料(陣列)\n",
    "\"\"\"\n",
    "#----------將標籤映射到CWB的顏色----------#\n",
    "import matplotlib\n",
    "nws_precip_colors = [\n",
    "    \"#fdfdfd\",  # (253,253,253)   #white \n",
    "    \"#c1c1c1\",  # (193,193,193)  #grays \n",
    "    \"#99ffff\",  # (153,255,255)   #blue \n",
    "    \"#00ccff\",  # (0,204,255)\n",
    "    \"#0099ff\",  # (0,153,255)\n",
    "    \"#0166ff\",  # (1,102,255)\n",
    "    \"#329900\",  # (50,153,0)        #green\n",
    "    \"#33ff00\",  # (51,255,0)\n",
    "    \"#ffff00\",  # (255,255,0)       #yellow\n",
    "    \"#ffcc00\",  # (255,204,0)\n",
    "    \"#fe9900\",  # (254,153,0)  \n",
    "    \"#fe0000\",  # (254,0,0)          #red\n",
    "    \"#cc0001\",  # (204,0,1)\n",
    "    \"#990000\",  # (153,0,0)\n",
    "    \"#990099\",  # (153,0,153)     #purple\n",
    "    \"#cb00cc\",  # (203,0,203)\n",
    "    \"#ff00fe\",  # (255,0,254)\n",
    "    \"#feccff\"   # (254,204,255)\n",
    "]\n",
    "precip_colormap = matplotlib.colors.ListedColormap(nws_precip_colors)\n",
    "eplison = 1e-7 #將所有標籤加上一個極小值，防止標籤=1的情況判斷成2\n",
    "#(因為0.1<=x<1；1<=x<2，所以標籤1會歸類成2，也就是灰色變成淺藍色了)\n",
    "clevels = [0,0.1, 1+eplison, 2+eplison, 6+eplison, 10+eplison, 15+eplison, 20+eplison\n",
    "           , 30+eplison, 40+eplison, 50+eplison, 70+eplison, 90+eplison,110+eplison\n",
    "           ,130+eplison,150+eplison, 200+eplison,300+eplison,500+eplison]  #自定義顏色列表\n",
    "norm = matplotlib.colors.BoundaryNorm(clevels, 18)\n",
    "CWB18_weight = [0,1,2,4,8,13,18,25,35,45,60,80,100,120,140,175,250,400]\n",
    "#----------計算累積雨量和RMSE----------#\n",
    "val = pd.read_csv('./database/train_list/'+t2t+'/val.txt',header=None,delimiter=';') #讀取val資料夾用於對應計算\n",
    "pre=val[:][0]  #在輸出預測已經將t->t+1的情形考慮進去，輸出了檔名皆對應 \n",
    "lab=val[:][1]  #標籤的檔名於第1行中\n",
    "pre_list=[]  \n",
    "data = np.zeros((len(pre),4), dtype=float)              #建立儲存rmse使用的陣列\n",
    "for k in range(len(pre)):\n",
    "    pre_tmp = pre[k] #取出第k個標籤，取出名稱並加上t+shift.png\n",
    "    pre_tmp = pre_tmp[:-4]+'_t+'+str(shift)+'.csv'  #shift需同predict，參照上格or資料夾\n",
    "    pre_list.append(pre_tmp) #將預測的名稱存入清單中\n",
    "    predict = np.genfromtxt('./output/predict_array/'+pre_tmp,delimiter=',') #匯入預測出來的圖像\n",
    "    #----------將標籤值映射到相對真值----------#\n",
    "    if interval == 'EWB10':  #如果情況為EWB10則啟用下列計算*5-5 回推相對真值\n",
    "        predict = predict *10-5  #思路:以10為間隔所以可以直接*10再減去5等於相對真值\n",
    "        predict[predict <= 0] = 0  #用於將0值變成負號(-5)得情況轉成0\n",
    "    if interval =='CWB18':\n",
    "        for i in range(17,-1,-1):  #倒序判斷，從大的標籤往小的去取\n",
    "            #由小到大會有問題(if 標籤4=>8，因為判斷還會繼續，所以原本4轉8，8就還會再轉成35，造成判斷錯誤\n",
    "            predict = np.where(predict==i,CWB18_weight[i],predict) #用於將CWB18的標籤值轉為相對真值\n",
    "    #----------轉換成CWB的圖像化----------#\n",
    "    plt.figure(figsize=(1,1)) #改變圖片尺寸(英吋)   #1英吋對應dpi pixel \n",
    "    plt.imshow(predict,cmap=precip_colormap, alpha=1, norm=norm)  #alpha為透明度\n",
    "    plt.axis('off')\n",
    "    plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0, hspace = 0, wspace = 0)  #控制子圖邊框\n",
    "    plt.savefig(\"./output/predict/\"+pre_tmp[:-4]+'.png',dpi=128,pad_inches=0.0) \n",
    "    plt.close()  #關閉圖像\n",
    "    \n",
    "    lab_tmp = lab[k] #取出第k個標籤\n",
    "    lab_tmp = lab_tmp[:-4]+'.csv'  #將標籤的檔名後四位刪除 (刪除.png) 因為標籤為csv形式\n",
    "    label = np.genfromtxt('./database/val_label_csv_true_value/'+lab_tmp,delimiter=',') #匯入標籤csv\n",
    "    label = label[1:,:]  #除去第一列不要的標頭\n",
    "    average_rmse=0  #(本島)平均均方根誤差\n",
    "    blank_rmse=0      #計算標籤和全白的預測的誤差(=label-0)  (搞不好全白的預測還比我們的好XD)\n",
    "    sum_pre=0 #本島預測累積雨量\n",
    "    sum_lab=0 #本島真實累積雨量\n",
    "    bias=predict-label        #絕對誤差\n",
    "    \n",
    "    sum_pre  = np.sum(predict)  #上方加總功能若是不管外邊，單純標籤相加減計算\n",
    "    sum_lab  = np.sum(label)\n",
    "    average_rmse = np.sum(bias**2)\n",
    "    blank_rmse = np.sum(label**2)\n",
    "    average_rmse = (average_rmse/4700)**0.5  #台灣本島內共有4700格\n",
    "    blank_rmse = (blank_rmse/4700)**0.5          \n",
    "    #print(average_rmse)   #標籤與預測圖像的誤差 (全島加總)\n",
    "    #print(blank_rmse)       #標籤與全白圖像的誤差  \n",
    "    #blank_rmse的意義為如果預測為全白的圖像，他的誤差會是多少?如果預測結果比這個還差是不是有問題\n",
    "    \n",
    "    data[k,0]=sum_pre/4700  #雨量加總除以4700代表臺灣本島平均降雨量\n",
    "    data[k,1]=sum_lab/4700\n",
    "    data[k,2]=average_rmse\n",
    "    data[k,3]=blank_rmse\n",
    "\n",
    "#----------輸出平均降雨量&RMSE----------#\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "plt.figure(12, figsize=(12, 10), dpi=60)\n",
    "plt.subplot(211)\n",
    "plt.plot(data[:,0], label='predict')\n",
    "plt.plot(data[:,1], label='label')\n",
    "plt.title('all point average rainfall')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(data[:,2], label='pre_RMSE')\n",
    "plt.plot(data[:,3], label='blank_RMSE')\n",
    "plt.title('RMSE')\n",
    "plt.legend()\n",
    "pre_sum = sum(data[:,0])\n",
    "label_sum = sum(data[:,1])\n",
    "pre_rmse = sum(data[:,2])/len(pre)\n",
    "blank_rmse = sum(data[:,3])/len(pre)\n",
    "print(pre_sum) #全台平均降雨量(預測)\n",
    "print(label_sum) #全台平均降雨量(真實)\n",
    "print(pre_rmse) #全台平均rmse(預測)\n",
    "print(blank_rmse) #全台平均rmse(真實-空白)\n",
    "header=['pre_sum','label_sum','pre_rmse','blank_rmse','time']\n",
    "df=pd.DataFrame(data)\n",
    "df.insert(df.shape[1],'time' , pre_list)\n",
    "df.to_csv('./output/model_pre_sum_rmse-'+str(round(pre_rmse,4))+'_'+t2t+'_'+interval+'_'+choose_model+'.csv', index=True, header=header, sep=',')\n",
    "plt.savefig('./output/model_pre_sum_rmse-'+str(round(pre_rmse,4))+'_'+t2t+'_'+interval+'_'+choose_model+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
